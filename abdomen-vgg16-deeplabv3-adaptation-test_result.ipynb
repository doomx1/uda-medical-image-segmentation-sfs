{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-End training and testing of the new pytorch-based VGG16-deeplabv3 architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/37893755/tensorflow-set-cuda-visible-devices-within-jupyter\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "# https://stackoverflow.com/questions/56008683/could-not-create-cudnn-handle-cudnn-status-internal-error\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.95)\n",
    "config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import wasserstein_utils\n",
    "import data_utils\n",
    "import evaluate\n",
    "import losses\n",
    "import networks\n",
    "import deeplabv3 as dlv3\n",
    "import utils\n",
    "import io_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = 'vgg16'\n",
    "dataset = \"abdomen\"\n",
    "\n",
    "# H x W x C\n",
    "img_shape = (256,256,3)\n",
    "\n",
    "# 4 classes + void\n",
    "num_classes = 5\n",
    "\n",
    "batch_size=16\n",
    "\n",
    "epochs=30000\n",
    "epoch_step=500\n",
    "\n",
    "num_projections=100\n",
    "\n",
    "data_dir = \"./data/abdomen/processed-data/\"\n",
    "source_train_list = io_utils.read_list_file(\"./data/abdomen/processed-data/mri_chaos_train_list\")\n",
    "source_val_list = io_utils.read_list_file(\"./data/abdomen/processed-data/mri_chaos_val_list\")\n",
    "target_list = io_utils.read_list_file(\"./data/abdomen/processed-data/ct_atlas_train_list\")\n",
    "target_test_list = io_utils.read_list_file(\"./data/abdomen/processed-data/ct_atlas_test_list\")\n",
    "\n",
    "suffix = \"run1\"\n",
    "fn_w_dlv3 = \"weights/\" + dataset + \"/\" + backbone + \"_deeplabv3_debug_\" + suffix + \".h5\"\n",
    "fn_w_cls = \"weights/\" + dataset + \"/\" + backbone + \"_deeplabv3_classifier_debug_\" + suffix + \".h5\"\n",
    "\n",
    "fn_w_adapted_dlv3 = \"weights/\" + dataset + \"/\" + backbone +\"_deeplabv3_adapted_debug_\" + suffix + \".h5\"\n",
    "fn_w_adapted_cls = \"weights/\" + dataset + \"/\" + backbone + \"_deeplabv3_classifier_adapted_debug_\" + suffix + \".h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(losses)\n",
    "importlib.reload(dlv3)\n",
    "\n",
    "deeplabv3 = dlv3.deeplabv3(activation=None, \\\n",
    "                           backbone=backbone, \\\n",
    "                           num_classes=num_classes, \\\n",
    "                           regularizer=tf.keras.regularizers.l2(1), \\\n",
    "                           dropout=.5)\n",
    "\n",
    "X = deeplabv3.input\n",
    "Y = tf.keras.layers.Input(shape=(img_shape[0], img_shape[1], num_classes,), dtype='float32', name='label_input')\n",
    "\n",
    "C_in = tf.keras.layers.Input(shape=deeplabv3.layers[-1].output_shape[1:], dtype='float32', name='classifier_input')\n",
    "classifier = tf.keras.Model(C_in, networks.classifier_layers(C_in, num_classes = num_classes, activation='softmax'))\n",
    "\n",
    "# A combined model, giving the output of classifier(deeplabv3(X))\n",
    "combined = tf.keras.Model(X, classifier(deeplabv3(X)))\n",
    "combined.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False))\n",
    "\n",
    "# A model outputting hxwx1 labels for each image. Also useful to verify the\n",
    "# mIoU with Keras' built-in function. Will however also consider the 'ignore' class. \n",
    "combined_ = tf.keras.Model(X, tf.cast(tf.keras.backend.argmax(combined(X), axis=-1), 'float32'))\n",
    "combined_.compile(metrics=[tf.keras.metrics.MeanIoU(num_classes=num_classes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabv3.load_weights(fn_w_dlv3)\n",
    "classifier.load_weights(fn_w_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Loading pre-learnt gaussians\")\n",
    "    \n",
    "    means = np.load(\"./extras/means_\" + backbone + \"_deeplabv3_\" + dataset + \"_\" + suffix + \".npy\")\n",
    "    covs = np.load(\"./extras/covs_\" + backbone + \"_deeplabv3_\" + dataset + \"_\" + suffix + \".npy\")\n",
    "except:\n",
    "    print(\"Learning means and covariances\")\n",
    "    \n",
    "    importlib.reload(utils)\n",
    "\n",
    "    # means = np.load(\"./extras/submission/means_\" + backbone + \"_deeplabv3_\" + dataset + \"_\" + suffix + \".npy\")\n",
    "    # covs = np.load(\"./extras/submission/covs_\" + backbone + \"_deeplabv3_\" + dataset + \"_\" + suffix + \".npy\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    means, _, ct = utils.learn_gaussians(data_dir, source_train_list, deeplabv3, combined, batch_size, data_utils.label_ids_abdomen, \\\n",
    "                                    rho=.97)\n",
    "    print(\"computed means in\", time.time() - start_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "    means, covs, ct = utils.learn_gaussians(data_dir, source_train_list, deeplabv3, combined, batch_size, data_utils.label_ids_abdomen, \\\n",
    "                                  rho=.97, initial_means=means)\n",
    "    print(\"finished training gaussians in\", time.time() - start_time)\n",
    "\n",
    "    np.save(\"./extras/means_\" + backbone + \"_deeplabv3_\" + dataset + \"_\" + suffix + \".npy\", means)\n",
    "    np.save(\"./extras/covs_\" + backbone + \"_deeplabv3_\" + dataset + \"_\" + suffix + \".npy\", covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data from the gmm model and plot it\n",
    "importlib.reload(umap)\n",
    "\n",
    "importlib.reload(utils)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "n_samples = np.ones(num_classes, dtype=int)\n",
    "n_samples *= 2000\n",
    "\n",
    "xx, yy = utils.sample_from_gaussians(means, covs, n_samples=n_samples)\n",
    "\n",
    "NUM_COLORS = num_classes\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "\n",
    "umap_embedding = reducer.fit_transform(xx)\n",
    "\n",
    "plt.figure(figsize=(16,14))\n",
    "cm = plt.get_cmap('gist_rainbow')\n",
    "\n",
    "shift = 1 / len(data_utils.label_ids_abdomen.keys())\n",
    "idx = 0\n",
    "for label in data_utils.label_ids_abdomen:\n",
    "    ind = yy == data_utils.label_ids_abdomen[label]\n",
    "    \n",
    "    plt.scatter(umap_embedding[:,0][ind], umap_embedding[:,1][ind], label=label, \\\n",
    "                color=cm(1.*idx/NUM_COLORS))\n",
    "    idx += 1\n",
    "\n",
    "plt.title(\"Embedding scatter-plot\")\n",
    "plt.legend()\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(wasserstein_utils)\n",
    "\n",
    "Z_s = tf.keras.layers.Input(shape=(img_shape[0], img_shape[1], num_classes,) )\n",
    "Y_s = tf.keras.backend.placeholder(shape=(None, img_shape[0], img_shape[1], num_classes), dtype='float32') #labels of input images oneHot\n",
    "lambda2 = .5\n",
    "\n",
    "loss_function = losses.masked_ce_loss(num_classes, None)\n",
    "wce_loss = loss_function(Y_s, classifier(Z_s), from_logits=False)\n",
    "\n",
    "# Wasserstein matcing loss\n",
    "theta = tf.keras.backend.placeholder(shape = (num_projections, num_classes), dtype='float32')\n",
    "matching_loss = wasserstein_utils.sWasserstein_hd(deeplabv3(X), Z_s, theta, nclass=num_classes, Cp=None, Cq=None,)\n",
    "\n",
    "# Overall loss is a weighted combination of the two losses\n",
    "total_loss = wce_loss + lambda2*matching_loss\n",
    "\n",
    "params = deeplabv3.trainable_weights + classifier.trainable_weights\n",
    "\n",
    "# Optimizer and training setup\n",
    "opt = tf.keras.optimizers.Adam(lr=5e-5, epsilon=1e-1, decay=1e-6)\n",
    "\n",
    "updates = opt.get_updates(total_loss, params)\n",
    "train = tf.keras.backend.function(inputs=[X,Z_s,Y_s,theta], outputs=[total_loss, wce_loss, matching_loss], updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(wasserstein_utils)\n",
    "\n",
    "Z_s = tf.keras.layers.Input(shape=(img_shape[0], img_shape[1], num_classes,) )\n",
    "Y_s = tf.keras.backend.placeholder(shape=(None, img_shape[0], img_shape[1], num_classes), dtype='float32') #labels of input images oneHot\n",
    "lambda2 = .5\n",
    "\n",
    "loss_function = losses.masked_ce_loss(num_classes, None)\n",
    "wce_loss = loss_function(Y_s, classifier(Z_s), from_logits=False)\n",
    "\n",
    "# Wasserstein matching loss\n",
    "theta = tf.keras.backend.placeholder(shape = (num_projections, num_classes), dtype='float32')\n",
    "matching_loss = wasserstein_utils.sWasserstein_hd(deeplabv3(X), Z_s, theta, nclass=num_classes, Cp=None, Cq=None,)\n",
    "\n",
    "# Overall loss is a weighted combination of the two losses\n",
    "total_loss = wce_loss + lambda2*matching_loss\n",
    "\n",
    "params = deeplabv3.trainable_weights + classifier.trainable_weights\n",
    "\n",
    "# Optimizer and training setup\n",
    "opt = tf.keras.optimizers.Adam(lr=5e-5, epsilon=1e-1, decay=1e-6)\n",
    "\n",
    "updates = opt.get_updates(total_loss, params)\n",
    "train = tf.keras.backend.function(inputs=[X,Z_s,Y_s,theta], outputs=[total_loss, wce_loss, matching_loss], updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "target_miou = []\n",
    "\n",
    "deeplabv3.load_weights(fn_w_dlv3)\n",
    "classifier.load_weights(fn_w_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(2,figsize=(15,10))\n",
    "\n",
    "for itr in range(epochs):\n",
    "    target_train_data, target_train_labels = io_utils.sample_batch(data_dir, target_list, \\\n",
    "                                                                   batch_size=batch_size, seed=itr)\n",
    "    \n",
    "    # make sure the #samples from gaussians match the distribution of the labels\n",
    "    n_samples = np.zeros(num_classes, dtype=int)\n",
    "    cls, ns = np.unique(target_train_labels, return_counts=True)\n",
    "    for i in range(len(cls)):\n",
    "        n_samples[cls[i]] = ns[i]\n",
    "\n",
    "    if np.sum(n_samples) % np.prod(img_shape) != 0:\n",
    "        remaining = np.prod(img_shape[:-1]) - np.sum(n_samples) % np.prod(img_shape[:-1])\n",
    "\n",
    "        aux = np.copy(n_samples) / np.sum(n_samples)\n",
    "        aux *= remaining\n",
    "        aux = np.floor(aux).astype('int')\n",
    "        \n",
    "        n_samples += aux\n",
    "        \n",
    "        # in case there are extra samples left, dump them on the highest represented class\n",
    "        n_samples[np.argmax(n_samples)] += remaining - np.sum(aux)\n",
    "\n",
    "    Yembed,Yembedlabels = utils.sample_from_gaussians(means, covs, n_samples = n_samples)\n",
    "    Yembed = Yembed.reshape(-1, img_shape[0], img_shape[1], num_classes)\n",
    "    Yembedlabels = Yembedlabels.reshape(-1, img_shape[0], img_shape[1])\n",
    "\n",
    "    Yembedlabels = tf.keras.utils.to_categorical(Yembedlabels, num_classes=num_classes)\n",
    "\n",
    "    theta_instance = tf.keras.backend.variable(wasserstein_utils.generateTheta(num_projections,num_classes))\n",
    "    loss.append(train(inputs=[target_train_data, Yembed, Yembedlabels, theta_instance]))\n",
    "    \n",
    "    target_miou.append(combined_.evaluate(target_train_data, target_train_labels, verbose=False)[-1])\n",
    "    \n",
    "    if itr%epoch_step==0 or itr < 1000:\n",
    "        deeplabv3.save_weights(fn_w_adapted_dlv3)\n",
    "        classifier.save_weights(fn_w_adapted_cls)\n",
    "        \n",
    "        # Debug info. First, the mIoU. Second, the categorical CE loss (ignoring class weights and containing) \n",
    "        # the ignore class\n",
    "        if itr != 0:\n",
    "            ax[0].clear()\n",
    "            \n",
    "            ll = np.asarray(loss)\n",
    "            ax[0].plot(np.log(ll[:,0]), label='log total loss')\n",
    "            ax[0].plot(np.log(ll[:,1]), label='log ce loss')\n",
    "            ax[0].plot(np.log(ll[:,2] * lambda2), label='log wasserstein loss')\n",
    "            ax[0].legend()\n",
    "            \n",
    "        ax[0].set_title(\"Log Loss\")\n",
    "        ax[0].set_xlabel(\"Epochs\")\n",
    "        ax[0].set_ylabel(\"Log Loss\")\n",
    "        \n",
    "        if itr != 0:\n",
    "            ax[1].clear()\n",
    "            ax[1].plot(np.asarray(target_miou))\n",
    "        \n",
    "        ax[1].set_title(\"MIOU on target domain\")\n",
    "        ax[1].set_xlabel(\"Epochs\")\n",
    "        ax[1].set_ylabel(\"Mean IOU\")\n",
    "        \n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        time.sleep(1e-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabv3.save_weights(fn_w_adapted_dlv3)\n",
    "classifier.save_weights(fn_w_adapted_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "target_cat_dice,target_dice = utils.compute_dice(data_dir, target_test_list, combined_, data_utils.label_ids_abdomen, \\\n",
    "                                                 id_to_ignore=0)\n",
    "\n",
    "for k in target_cat_dice:\n",
    "    print(k, target_cat_dice[k])\n",
    "print(target_dice)\n",
    "\n",
    "print('Computed ' + dataset + ' target DICE in', time.time() - start_time)\n",
    "\n",
    "# liver 0.8745730727705845\n",
    "# right_kidney 0.7234397414832942\n",
    "# left_kidney 0.8030892885012225\n",
    "# spleen 0.8031550976644446\n",
    "# 0.8010643001048865\n",
    "# Computed abdomen target DICE in 23.268261671066284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "target_cat_dice,target_dice = utils.compute_dice(data_dir, target_list, combined_, data_utils.label_ids_abdomen, \\\n",
    "                                                 id_to_ignore=0)\n",
    "\n",
    "for k in target_cat_dice:\n",
    "    print(k, target_cat_dice[k])\n",
    "print(target_dice)\n",
    "\n",
    "print('Computed ' + dataset + ' target DICE in', time.time() - start_time)\n",
    "\n",
    "# liver 0.8783129334054984\n",
    "# right_kidney 0.7251318294154707\n",
    "# left_kidney 0.782384974705287\n",
    "# spleen 0.8428680590576928\n",
    "# 0.8071744491459872\n",
    "# Computed abdomen target DICE in 991.6162948608398"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
