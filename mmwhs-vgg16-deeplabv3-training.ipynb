{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain best possible source domain training results (above .82 dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/37893755/tensorflow-set-cuda-visible-devices-within-jupyter\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# https://stackoverflow.com/questions/56008683/could-not-create-cudnn-handle-cudnn-status-internal-error\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.95)\n",
    "config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import wasserstein_utils\n",
    "import data_utils\n",
    "import losses\n",
    "import networks\n",
    "import deeplabv3 as dlv3\n",
    "import utils\n",
    "import io_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = 'vgg16'\n",
    "dataset = \"mmwhs\"\n",
    "\n",
    "# H x W x C\n",
    "img_shape = (256,256,3)\n",
    "\n",
    "# 4 classes + void\n",
    "num_classes = 5\n",
    "\n",
    "batch_size=16\n",
    "\n",
    "do_training = True\n",
    "\n",
    "epochs=90000\n",
    "epoch_step=250\n",
    "\n",
    "num_projections=100\n",
    "\n",
    "data_dir = \"./data/mmwhs/PnpAda_release_data/train&val/\"\n",
    "source_list = io_utils.read_list_file(\"./data/mmwhs/PnpAda_release_data/train&val/mr_train_list\")\n",
    "source_val_list = io_utils.read_list_file(\"./data/mmwhs/PnpAda_release_data/train&val/mr_val_list\")\n",
    "target_list = io_utils.read_list_file(\"./data/mmwhs/PnpAda_release_data/train&val/ct_train_list\")\n",
    "target_val_list = io_utils.read_list_file(\"./data/mmwhs/PnpAda_release_data/train&val/ct_val_list\")\n",
    "target_test_list = io_utils.read_list_file(\"./data/mmwhs/PnpAda_release_data/train&val/ct_test_list\")\n",
    "\n",
    "fn_w_dlv3 = \"weights/\" + dataset + \"/\" + backbone + \"_deeplabv3_debug.h5\"\n",
    "fn_w_cls = \"weights/\" + dataset + \"/\" + backbone + \"_deeplabv3_classifier_debug.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(losses)\n",
    "\n",
    "deeplabv3 = dlv3.deeplabv3(activation=None, \\\n",
    "                           backbone=backbone, \\\n",
    "                           num_classes=num_classes, \\\n",
    "                           regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "\n",
    "X = deeplabv3.input\n",
    "Y = tf.keras.layers.Input(shape=(img_shape[0], img_shape[1], num_classes,), dtype='float32', name='label_input')\n",
    "\n",
    "C_in = tf.keras.layers.Input(shape=deeplabv3.layers[-1].output_shape[1:], dtype='float32', name='classifier_input')\n",
    "classifier = tf.keras.Model(C_in, networks.classifier_layers(C_in, num_classes = num_classes, activation='softmax'))\n",
    "\n",
    "# A combined model, giving the output of classifier(deeplabv3(X))\n",
    "combined = tf.keras.Model(X, classifier(deeplabv3(X)))\n",
    "combined.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False))\n",
    "\n",
    "# A model outputting hxwx1 labels for each image. Also useful to verify the\n",
    "# mIoU with Keras' built-in function. Will however also consider the 'ignore' class. \n",
    "combined_ = tf.keras.Model(X, tf.cast(tf.keras.backend.argmax(combined(X), axis=-1), 'float32'))\n",
    "combined_.compile(metrics=[tf.keras.metrics.MeanIoU(num_classes=num_classes)])\n",
    "\n",
    "# Set up the loss functions\n",
    "loss_function = losses.masked_ce_loss(num_classes, None)\n",
    "wce_loss = loss_function(Y, classifier(deeplabv3(X)), from_logits=False)\n",
    "\n",
    "# Set up training\n",
    "opt = [tf.keras.optimizers.Adam(lr=1e-4, epsilon=1e-6, decay=1e-6), \\\n",
    "      tf.keras.optimizers.Adam(lr=1e-4, epsilon=1e-6, decay=1e-6), \\\n",
    "      tf.keras.optimizers.Adam(lr=1e-4, epsilon=1e-6, decay=1e-6)]\n",
    "\n",
    "# https://stackoverflow.com/questions/55434653/batch-normalization-doesnt-have-gradient-in-tensorflow-2-0\n",
    "params = deeplabv3.trainable_weights + classifier.trainable_weights\n",
    "\n",
    "updates = [o.get_updates(wce_loss, params) for o in opt]\n",
    "train = [tf.keras.backend.function(inputs=[X,Y], outputs=[wce_loss], updates=u) for u in updates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "importlib.reload(io_utils)\n",
    "\n",
    "# Training on source domain\n",
    "if do_training == True:\n",
    "    try:\n",
    "#         deeplabv3.load_weights(fn_w_dlv3)\n",
    "#         classifier.load_weights(fn_w_cls)\n",
    "\n",
    "#         print(\"Successfully loaded model. Continuing training.\")\n",
    "        print(\"Training from scratch\")\n",
    "    except:\n",
    "        print(\"Could not load previous model weights. Is a new model present?\")\n",
    "        \n",
    "    start_time = time.time()\n",
    "    fig,ax = plt.subplots(1,figsize=(10,7))\n",
    "    loss_history = []\n",
    "    \n",
    "    for itr in range(epochs):\n",
    "        opt_idx = itr // (epochs // len(train) + 1)\n",
    "        \n",
    "        source_train_data, source_train_labels = io_utils.sample_batch(data_dir, source_list, \\\n",
    "                                                                       batch_size=batch_size, seed=itr)\n",
    "        source_train_labels = tf.keras.utils.to_categorical(source_train_labels, num_classes=num_classes)\n",
    "        \n",
    "        loss_history.append(train[opt_idx](inputs=[ source_train_data, source_train_labels ]))\n",
    "\n",
    "        if itr%epoch_step == 0 or itr < 1000:\n",
    "            if itr != 0:\n",
    "                ax.clear()\n",
    "                ax.plot(np.log(np.asarray(loss_history)))\n",
    "\n",
    "            ax.set_title(\"Training loss on source domain\")\n",
    "            ax.set_xlabel(\"Epoch\")\n",
    "            ax.set_ylabel(\"Log Loss\")\n",
    "\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            time.sleep(1e-3)\n",
    "            \n",
    "        if itr % (epochs // 10) == 0 or itr == epochs - 1:\n",
    "            deeplabv3.save_weights(fn_w_dlv3)\n",
    "            classifier.save_weights(fn_w_cls)\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "else:\n",
    "    deeplabv3.load_weights(fn_w_dlv3)\n",
    "    classifier.load_weights(fn_w_cls)\n",
    "    print(\"Loaded model weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train_data, source_train_labels = io_utils.sample_batch(data_dir, source_val_list, \\\n",
    "                                                               batch_size=batch_size, seed=42)\n",
    "\n",
    "print(source_train_data.shape, source_train_labels.shape)\n",
    "idx = 10\n",
    "\n",
    "plt.imshow(source_train_data[idx][:,:,1])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(source_train_labels[idx].reshape(256,256), vmin=0, vmax=4)\n",
    "plt.show()\n",
    "\n",
    "myans = combined_.predict(source_train_data[idx].reshape(1,256,256,3))\n",
    "plt.imshow(myans[0], vmin=0, vmax=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "importlib.reload(data_utils)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "source_cat_iou,source_miou = utils.compute_miou(data_dir, source_val_list, combined_, data_utils.label_ids_mmwhs, \\\n",
    "                                                id_to_ignore=0)\n",
    "\n",
    "for k in source_cat_iou:\n",
    "    print(k, source_cat_iou[k])\n",
    "print(source_miou)\n",
    "\n",
    "print('Computed ' + dataset + ' mIoU in', time.time() - start_time)\n",
    "\n",
    "# lv_myo 0.6391655877566917\n",
    "# la_blood 0.7761180002868887\n",
    "# lv_blood 0.8552707982717727\n",
    "# aa 0.6573924014969161\n",
    "# 0.7319866969530673\n",
    "# Computed mmwhs mIoU in 198.71120285987854"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "importlib.reload(data_utils)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "source_cat_dice,source_dice = utils.compute_dice(data_dir, source_val_list, combined_, data_utils.label_ids_mmwhs, \\\n",
    "                                                 id_to_ignore=0)\n",
    "\n",
    "for k in source_cat_dice:\n",
    "    print(k, source_cat_dice[k])\n",
    "print(source_dice)\n",
    "\n",
    "print('Computed ' + dataset + ' DICE in', time.time() - start_time)\n",
    "\n",
    "# lv_myo 0.7798670159143992\n",
    "# la_blood 0.8739486905279106\n",
    "# lv_blood 0.9219902550813358\n",
    "# aa 0.7932851639758641\n",
    "# 0.8422727813748774\n",
    "# Computed mmwhs DICE in 199.5630497932434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "importlib.reload(data_utils)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "target_cat_dice,target_dice = utils.compute_dice(data_dir, target_val_list, combined_, data_utils.label_ids_mmwhs, \\\n",
    "                                                 id_to_ignore=0)\n",
    "\n",
    "for k in target_cat_dice:\n",
    "    print(k, target_cat_dice[k])\n",
    "print(target_dice)\n",
    "\n",
    "print('Computed ' + dataset + ' target DICE in', time.time() - start_time)\n",
    "\n",
    "# lv_myo 0.1805723386505549\n",
    "# la_blood 0.732765818219422\n",
    "# lv_blood 0.274775071361346\n",
    "# aa 0.6930902059047827\n",
    "# 0.47030085853402637\n",
    "# Computed mmwhs target DICE in 98.94450497627258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "importlib.reload(data_utils)\n",
    "importlib.reload(io_utils)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "target_cat_dice,target_dice = utils.compute_dice(data_dir, target_test_list, combined_, data_utils.label_ids_mmwhs, \\\n",
    "                                                 id_to_ignore=0)\n",
    "\n",
    "for k in target_cat_dice:\n",
    "    print(k, target_cat_dice[k])\n",
    "print(target_dice)\n",
    "\n",
    "print('Computed ' + dataset + ' target DICE in', time.time() - start_time)\n",
    "\n",
    "# lv_myo 0.30680336428773775\n",
    "# la_blood 0.8255339521423518\n",
    "# lv_blood 0.47421624761212994\n",
    "# aa 0.7822472557192597\n",
    "# 0.5972002049403697\n",
    "# Computed mmwhs target DICE in 82.1158561706543"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
