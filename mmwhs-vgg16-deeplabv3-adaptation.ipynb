{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/37893755/tensorflow-set-cuda-visible-devices-within-jupyter\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# https://stackoverflow.com/questions/56008683/could-not-create-cudnn-handle-cudnn-status-internal-error\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.95)\n",
    "config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import wasserstein_utils\n",
    "import data_utils\n",
    "import losses\n",
    "import networks\n",
    "import deeplabv3 as dlv3\n",
    "import utils\n",
    "import io_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = 'vgg16'\n",
    "dataset = \"mmwhs\"\n",
    "\n",
    "# H x W x C\n",
    "img_shape = (256,256,3)\n",
    "\n",
    "# 4 classes + void\n",
    "num_classes = 5\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "do_training = True\n",
    "\n",
    "epochs_adapt=35000\n",
    "epoch_step=250\n",
    "\n",
    "num_projections=100\n",
    "\n",
    "data_dir = \"./data/mmwhs/PnpAda_release_data/train&val/\"\n",
    "source_list = io_utils.read_list_file(\"./data/mmwhs/PnpAda_release_data/train&val/mr_train_list\")\n",
    "source_val_list = io_utils.read_list_file(\"./data/mmwhs/PnpAda_release_data/train&val/mr_val_list\")\n",
    "target_list = io_utils.read_list_file(\"./data/mmwhs/PnpAda_release_data/train&val/ct_train_list\")\n",
    "target_val_list = io_utils.read_list_file(\"./data/mmwhs/PnpAda_release_data/train&val/ct_val_list\")\n",
    "target_test_list = io_utils.read_list_file(\"./data/mmwhs/PnpAda_release_data/train&val/ct_test_list\")\n",
    "\n",
    "suffix = \"run1\"\n",
    "fn_w_dlv3 = \"weights/\" + dataset + \"/\" + backbone + \"_deeplabv3_debug_\" + suffix + \".h5\"\n",
    "fn_w_cls = \"weights/\" + dataset + \"/\" + backbone + \"_deeplabv3_classifier_debug_\" + suffix + \".h5\"\n",
    "\n",
    "fn_w_adapted_dlv3 = \"weights/\" + dataset + \"/\" + backbone +\"_deeplabv3_adapted_debug_\" + suffix + \".h5\"\n",
    "fn_w_adapted_cls = \"weights/\" + dataset + \"/\" + backbone + \"_deeplabv3_classifier_adapted_debug_\" + suffix + \".h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(losses)\n",
    "\n",
    "deeplabv3 = dlv3.deeplabv3(activation=None, \\\n",
    "                           backbone=backbone, \\\n",
    "                           num_classes=num_classes)\n",
    "\n",
    "X = deeplabv3.input\n",
    "Y = tf.keras.layers.Input(shape=(img_shape[0], img_shape[1], num_classes,), dtype='float32', name='label_input')\n",
    "\n",
    "C_in = tf.keras.layers.Input(shape=deeplabv3.layers[-1].output_shape[1:], dtype='float32', name='classifier_input')\n",
    "classifier = tf.keras.Model(C_in, networks.classifier_layers(C_in, num_classes = num_classes, activation='softmax'))\n",
    "\n",
    "# A combined model, giving the output of classifier(deeplabv3(X))\n",
    "combined = tf.keras.Model(X, classifier(deeplabv3(X)))\n",
    "combined.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False))\n",
    "\n",
    "# A model outputting hxwx1 labels for each image. Also useful to verify the\n",
    "# mIoU with Keras' built-in function. Will however also consider the 'ignore' class. \n",
    "combined_ = tf.keras.Model(X, tf.cast(tf.keras.backend.argmax(combined(X), axis=-1), 'float32'))\n",
    "combined_.compile(metrics=[tf.keras.metrics.MeanIoU(num_classes=num_classes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deeplabv3.load_weights(fn_w_dlv3)\n",
    "classifier.load_weights(fn_w_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    means = np.load(\"./extras/means_\" + backbone + \"_deeplabv3_\" + dataset + \"_\" + suffix + \".npy\")\n",
    "    covs = np.load(\"./extras/covs_\" + backbone + \"_deeplabv3_\" + dataset + \"_\" + suffix + \".npy\")\n",
    "except:\n",
    "    print(\"Learning means and covariances\")\n",
    "    \n",
    "    importlib.reload(utils)\n",
    "\n",
    "    # means = np.load(\"./extras/submission/means_\" + backbone + \"_deeplabv3_\" + dataset + \"_\" + suffix + \".npy\")\n",
    "    # covs = np.load(\"./extras/submission/covs_\" + backbone + \"_deeplabv3_\" + dataset + \"_\" + suffix + \".npy\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    means, _, ct = utils.learn_gaussians(data_dir, source_list, deeplabv3, combined, batch_size, data_utils.label_ids_mmwhs, \\\n",
    "                                        rho=.97)\n",
    "    print(\"computed means in\", time.time() - start_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "    means, covs, ct = utils.learn_gaussians(data_dir, source_list, deeplabv3, combined, batch_size, data_utils.label_ids_mmwhs, \\\n",
    "                                      rho=.97, initial_means=means)\n",
    "    print(\"finished training gaussians in\", time.time() - start_time)\n",
    "\n",
    "    np.save(\"./extras/means_\" + backbone + \"_deeplabv3_\" + dataset + \"_\" + suffix + \".npy\", means)\n",
    "    np.save(\"./extras/covs_\" + backbone + \"_deeplabv3_\" + dataset + \"_\" + suffix + \".npy\", covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data from the gmm model and plot it\n",
    "start_time = time.time()\n",
    "\n",
    "n_samples = np.ones(num_classes, dtype=int)\n",
    "n_samples *= 2000\n",
    "\n",
    "xx, yy = utils.sample_from_gaussians(means, covs, n_samples=n_samples)\n",
    "NUM_COLORS = num_classes\n",
    "reducer = umap.UMAP()\n",
    "umap_embedding = reducer.fit_transform(xx)\n",
    "\n",
    "plt.figure(figsize=(16,14))\n",
    "cm = plt.get_cmap('gist_rainbow')\n",
    "\n",
    "shift = 1 / len(data_utils.label_ids_mmwhs.keys())\n",
    "idx = 0\n",
    "for label in data_utils.label_ids_mmwhs:\n",
    "    ind = yy == data_utils.label_ids_mmwhs[label]\n",
    "    \n",
    "    plt.scatter(umap_embedding[:,0][ind], umap_embedding[:,1][ind], label=label, \\\n",
    "                color=cm(1.*idx/NUM_COLORS))\n",
    "    idx += 1\n",
    "\n",
    "plt.title(\"Embedding scatter-plot\")\n",
    "plt.legend()\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(wasserstein_utils)\n",
    "\n",
    "Z_s = tf.keras.layers.Input(shape=(img_shape[0], img_shape[1], num_classes,) )\n",
    "Y_s = tf.keras.backend.placeholder(shape=(None, img_shape[0], img_shape[1], num_classes), dtype='float32') #labels of input images oneHot\n",
    "lambda2 = .5\n",
    "\n",
    "loss_function = losses.masked_ce_loss(num_classes, None)\n",
    "wce_loss = loss_function(Y_s, classifier(Z_s), from_logits=False)\n",
    "\n",
    "# Wasserstein matcing loss\n",
    "theta = tf.keras.backend.placeholder(shape = (num_projections, num_classes), dtype='float32')\n",
    "matching_loss = wasserstein_utils.sWasserstein_hd(deeplabv3(X), Z_s, theta, nclass=num_classes, Cp=None, Cq=None,)\n",
    "\n",
    "# Overall loss is a weighted combination of the two losses\n",
    "total_loss = wce_loss + lambda2*matching_loss\n",
    "\n",
    "params = deeplabv3.trainable_weights + classifier.trainable_weights\n",
    "\n",
    "# Optimizer and training setup\n",
    "opt = tf.keras.optimizers.Adam(lr=5e-5, epsilon=1e-1, decay=1e-6)\n",
    "\n",
    "updates = opt.get_updates(total_loss, params)\n",
    "train = tf.keras.backend.function(inputs=[X,Z_s,Y_s,theta], outputs=[total_loss, wce_loss, matching_loss], updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "target_miou = []\n",
    "\n",
    "deeplabv3.load_weights(fn_w_dlv3)\n",
    "classifier.load_weights(fn_w_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_training == True:\n",
    "    print(\"Commencing adaptation training!\")\n",
    "    \n",
    "    fig,ax=plt.subplots(2,figsize=(15,10))\n",
    "\n",
    "    batch_size = 20\n",
    "\n",
    "    thres = .99\n",
    "    epochs_adapt = 30000\n",
    "    epochstep = 250\n",
    "\n",
    "    for itr in range(epochs_adapt):\n",
    "        target_train_data, target_train_labels = io_utils.sample_batch(data_dir, target_list, \\\n",
    "                                                                       batch_size=batch_size, seed=itr)\n",
    "\n",
    "        # make sure the #samples from gaussians match the distribution of the labels\n",
    "        n_samples = np.zeros(num_classes, dtype=int)\n",
    "        cls, ns = np.unique(target_train_labels, return_counts=True)\n",
    "        for i in range(len(cls)):\n",
    "            n_samples[cls[i]] = ns[i]\n",
    "\n",
    "        if np.sum(n_samples) % np.prod(img_shape) != 0:\n",
    "            remaining = np.prod(img_shape[:-1]) - np.sum(n_samples) % np.prod(img_shape[:-1])\n",
    "\n",
    "            aux = np.copy(n_samples) / np.sum(n_samples)\n",
    "            aux *= remaining\n",
    "            aux = np.floor(aux).astype('int')\n",
    "\n",
    "            n_samples += aux\n",
    "\n",
    "            # in case there are extra samples left, dump them on the highest represented class\n",
    "            n_samples[np.argmax(n_samples)] += remaining - np.sum(aux)\n",
    "\n",
    "        Yembed,Yembedlabels = utils.sample_from_gaussians(means, covs, n_samples = n_samples)\n",
    "        Yembed = Yembed.reshape(-1, img_shape[0], img_shape[1], num_classes)\n",
    "        Yembedlabels = Yembedlabels.reshape(-1, img_shape[0], img_shape[1])\n",
    "\n",
    "        Yembedlabels = tf.keras.utils.to_categorical(Yembedlabels, num_classes=num_classes)\n",
    "\n",
    "        theta_instance = tf.keras.backend.variable(wasserstein_utils.generateTheta(num_projections,num_classes))\n",
    "        loss.append(train(inputs=[target_train_data, Yembed, Yembedlabels, theta_instance]))\n",
    "\n",
    "        target_miou.append(combined_.evaluate(target_train_data, target_train_labels, verbose=False)[-1])\n",
    "\n",
    "        if itr%epochstep==0 or itr < 1000:\n",
    "            deeplabv3.save_weights(fn_w_adapted_dlv3)\n",
    "            classifier.save_weights(fn_w_adapted_cls)\n",
    "            \n",
    "            # Debug info. First, the mIoU. Second, the categorical CE loss (ignoring class weights and containing) \n",
    "            # the ignore class\n",
    "            if itr != 0:\n",
    "                ax[0].clear()\n",
    "\n",
    "                ll = np.asarray(loss)\n",
    "                ax[0].plot(np.log(ll[:,0]), label='log total loss')\n",
    "                ax[0].plot(np.log(ll[:,1]), label='log ce loss')\n",
    "                ax[0].plot(np.log(ll[:,2] * lambda2), label='log wasserstein loss')\n",
    "                ax[0].legend()\n",
    "\n",
    "            ax[0].set_title(\"Log Loss\")\n",
    "            ax[0].set_xlabel(\"Epochs\")\n",
    "            ax[0].set_ylabel(\"Log Loss\")\n",
    "\n",
    "            if itr != 0:\n",
    "                ax[1].clear()\n",
    "                ax[1].plot(np.asarray(target_miou))\n",
    "\n",
    "            ax[1].set_title(\"MIOU on target domain\")\n",
    "            ax[1].set_xlabel(\"Epochs\")\n",
    "            ax[1].set_ylabel(\"Mean IOU\")\n",
    "\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "            time.sleep(1e-3) \n",
    "else:\n",
    "    deeplabv3.load_weights(fn_w_adapted_dlv3)\n",
    "    classifier.load_weights(fn_w_adapted_cls)\n",
    "    print(\"No training, loaded model weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabv3.save_weights(fn_w_adapted_dlv3)\n",
    "classifier.save_weights(fn_w_adapted_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "importlib.reload(evaluate)\n",
    "\n",
    "dtm, dm, dsd, atm, am, asd = evaluate.evaluate_mmwhs(data_dir, target_test_list, combined_, data_utils.label_ids_mmwhs, \\\n",
    "                                                 id_to_ignore=0)\n",
    "\n",
    "print((100 * dtm).round(1), atm.round(2))\n",
    "# print(dtm, atm)\n",
    "for label in dm:\n",
    "    print(label, (100 * dm[label]).round(1), (100 * dsd[label]).round(1), am[label].round(2), asd[label].round(2))\n",
    "    \n",
    "# (256, 256, 256) (256, 256, 256)\n",
    "# (256, 256, 256) (256, 256, 256)\n",
    "# (256, 256, 256) (256, 256, 256)\n",
    "# (256, 256, 256) (256, 256, 256)\n",
    "# 79.0 9.76\n",
    "# lv_myo 70.9 3.6 8.05 1.79\n",
    "# la_blood 83.5 5.0 9.63 5.85\n",
    "# lv_blood 75.4 10.8 10.25 2.75\n",
    "# aa 86.2 2.7 11.11 3.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
